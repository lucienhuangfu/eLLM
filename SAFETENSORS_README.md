# Llama3 8B SafeTensors æ¨¡å‹åŠ è½½å™¨

è¿™ä¸ªé¡¹ç›®å®ç°äº†ç”¨RuståŠ è½½Llama3 8Bçš„safetensorsæ¨¡å‹æ ¼å¼çš„åŠŸèƒ½ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ æ”¯æŒå•æ–‡ä»¶å’Œå¤šæ–‡ä»¶safetensorsæ ¼å¼
- ğŸ’¾ é«˜æ•ˆçš„å†…å­˜æ˜ å°„åŠ è½½ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨
- ğŸ”„ æ”¯æŒF16ã€F32ã€BF16æ•°æ®ç±»å‹è‡ªåŠ¨è½¬æ¢åˆ°std::f16
- âœ… å®Œæ•´çš„æ¨¡å‹éªŒè¯å’Œå±‚æ£€æŸ¥
- ğŸ“Š è¯¦ç»†çš„å†…å­˜ä½¿ç”¨æƒ…å†µåˆ†æ

## ä¾èµ–é¡¹

åœ¨`Cargo.toml`ä¸­æ·»åŠ äº†ä»¥ä¸‹ä¾èµ–ï¼š

```toml
safetensors = "0.4.1"
memmap2 = "0.9.4"
half = { version = "2.4.1", features = ["num-traits", "std"] }
```

## ä¸»è¦ç»„ä»¶

### 1. SafeTensorsModelLoader
å•æ–‡ä»¶æ¨¡å‹åŠ è½½å™¨ï¼Œé€‚ç”¨äºå•ä¸ªsafetensorsæ–‡ä»¶çš„æ¨¡å‹ã€‚

### 2. MultiFileSafeTensorsLoader  
å¤šæ–‡ä»¶æ¨¡å‹åŠ è½½å™¨ï¼Œé€‚ç”¨äºåˆ†ç‰‡çš„safetensorsæ¨¡å‹ã€‚

### 3. ä¾¿æ°‘å‡½æ•°
`load_llama3_from_safetensors()` - è‡ªåŠ¨æ£€æµ‹å¹¶åŠ è½½å•æ–‡ä»¶æˆ–å¤šæ–‡ä»¶æ¨¡å‹ã€‚

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨

```rust
use eLLM::llama::model_loader::load_llama3_from_safetensors;

// åŠ è½½æ¨¡å‹
let (config, weights) = load_llama3_from_safetensors("path/to/llama3-8b-model")?;

println!("æ¨¡å‹ç±»å‹: {}", config.model_type);
println!("éšè—å±‚å¤§å°: {}", config.hidden_size);
println!("åŠ è½½çš„æƒé‡æ•°é‡: {}", weights.len());
```

### ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·

é¡¹ç›®åŒ…å«ä¸€ä¸ªå®Œæ•´çš„å‘½ä»¤è¡Œå·¥å…·æ¥æ¼”ç¤ºæ¨¡å‹åŠ è½½ï¼š

```bash
# ç¼–è¯‘é¡¹ç›®
cargo build --release

# è¿è¡ŒsafetensorsåŠ è½½å™¨
cargo run --bin safetensors_loader -- /path/to/your/llama3-8b-model

# æˆ–ä½¿ç”¨é»˜è®¤è·¯å¾„
cargo run --bin safetensors_loader
```

### è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹

```rust
use eLLM::llama::model_loader::SafeTensorsModelLoader;

// åˆ›å»ºåŠ è½½å™¨
let loader = SafeTensorsModelLoader::new("path/to/model")?;

// åˆ†åˆ«åŠ è½½é…ç½®å’Œæƒé‡
let config = loader.load_config()?;
let weights = loader.load_weights_f16()?;

// éªŒè¯æ¨¡å‹å®Œæ•´æ€§
for i in 0..config.num_hidden_layers {
    let q_proj = format!("model.layers.{}.self_attn.q_proj.weight", i);
    if weights.contains_key(&q_proj) {
        println!("Layer {} Q projection found", i);
    }
}
```

## æ¨¡å‹ç›®å½•ç»“æ„

æ¨¡å‹ç›®å½•åº”åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š

```
model_directory/
â”œâ”€â”€ config.json              # æ¨¡å‹é…ç½®æ–‡ä»¶
â”œâ”€â”€ model.safetensors        # å•æ–‡ä»¶æ¨¡å‹ (æˆ–)
â”œâ”€â”€ model-00001-of-00001.safetensors  # åˆ†ç‰‡æ–‡ä»¶
â”œâ”€â”€ model-00002-of-00001.safetensors
â””â”€â”€ ...
```

## æ”¯æŒçš„æ–‡ä»¶å‘½åæ¨¡å¼

åŠ è½½å™¨ä¼šè‡ªåŠ¨æŸ¥æ‰¾ä»¥ä¸‹å‘½åæ¨¡å¼çš„æ–‡ä»¶ï¼š
- `model.safetensors`
- `pytorch_model.safetensors`
- `model-00001-of-00001.safetensors`
- `model-*.safetensors` (åˆ†ç‰‡æ¨¡å¼)

## æ•°æ®ç±»å‹è½¬æ¢

- **F16**: ç›´æ¥åŠ è½½ä¸ºstd::f16
- **F32**: è½¬æ¢ä¸ºstd::f16 (ä½¿ç”¨asè½¬æ¢)
- **BF16**: å…ˆè½¬æ¢ä¸ºf32ï¼Œå†è½¬æ¢ä¸ºstd::f16

## å†…å­˜ä½¿ç”¨

- ä½¿ç”¨å†…å­˜æ˜ å°„(mmap)å‡å°‘å†…å­˜å ç”¨
- F16æ ¼å¼ä¸‹ï¼Œ8Bæ¨¡å‹çº¦å ç”¨16GBå†…å­˜
- æ”¯æŒå¤§å‹æ¨¡å‹çš„åˆ†ç‰‡åŠ è½½

## æ¨¡å‹éªŒè¯

åŠ è½½å™¨ä¼šè‡ªåŠ¨éªŒè¯ä»¥ä¸‹ç»„ä»¶ï¼š
- åŸºç¡€å±‚ï¼šembedding, norm, lm_head
- Transformerå±‚ï¼šattentionå’ŒMLPç»„ä»¶
- å‚æ•°å®Œæ•´æ€§æ£€æŸ¥

## ç¤ºä¾‹è¾“å‡º

```
Loading Llama3 8B model from: models/llama3-8b-instruct
Found model file: models/llama3-8b-instruct/model.safetensors
âœ… Successfully loaded Llama3 8B model!

ğŸ“Š Model Configuration:
  Model Type: llama
  Hidden Size: 4096
  Layers: 32
  Attention Heads: 32
  Key-Value Heads: 8
  Vocabulary Size: 128256
  Max Position Embeddings: 8192
  RMS Norm Epsilon: 0.00001

ğŸ’¾ Memory Usage:
  Total Parameters: 8.03B
  Memory Usage (f16): 16.06 GB
  Loaded Tensors: 291

ğŸ” Verifying Key Layers:
  âœ… model.embed_tokens.weight: 524550144 params
  âœ… model.norm.weight: 4096 params  
  âœ… lm_head.weight: 524550144 params
  âœ… Complete transformer layers: 32/32

ğŸ“ˆ Large Tensors:
  â€¢ model.embed_tokens.weight: 524.6M params (1049.1 MB)
  â€¢ lm_head.weight: 524.6M params (1049.1 MB)
  â€¢ model.layers.0.mlp.up_proj.weight: 45.1M params (90.2 MB)
  â€¢ model.layers.0.mlp.gate_proj.weight: 45.1M params (90.2 MB)
  â€¢ model.layers.0.mlp.down_proj.weight: 45.1M params (90.2 MB)

âœ… Model loading and verification completed!
```

## é”™è¯¯å¤„ç†

å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆï¼š

1. **"config.json not found"**: ç¡®ä¿æ¨¡å‹ç›®å½•åŒ…å«é…ç½®æ–‡ä»¶
2. **"No safetensors file found"**: æ£€æŸ¥safetensorsæ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å‘½åæ­£ç¡®
3. **"Unsupported tensor dtype"**: å½“å‰æ”¯æŒF16/F32/BF16ï¼Œå…¶ä»–æ ¼å¼éœ€è¦æ‰©å±•

## æ€§èƒ½ä¼˜åŒ–

- ä½¿ç”¨å†…å­˜æ˜ å°„é¿å…å®Œæ•´åŠ è½½åˆ°å†…å­˜
- æ”¯æŒå¹¶è¡ŒåŠ è½½å¤šä¸ªæ–‡ä»¶
- å»¶è¿ŸåŠ è½½ï¼Œåªåœ¨éœ€è¦æ—¶è¯»å–æ•°æ®

## æ‰©å±•åŠŸèƒ½

å¯ä»¥åŸºäºè¿™ä¸ªåŠ è½½å™¨å®ç°ï¼š
- æ¨¡å‹é‡åŒ–
- åŠ¨æ€æ‰¹å¤„ç†
- GPUåŠ é€Ÿæ¨ç†
- æµå¼ç”Ÿæˆ

## æ³¨æ„äº‹é¡¹

1. éœ€è¦å¯ç”¨nightly Rustç‰¹æ€§ `#![feature(f16)]`
2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å­˜åŠ è½½8Bæ¨¡å‹
3. safetensorsæ–‡ä»¶éœ€è¦æ˜¯æœ‰æ•ˆçš„æ ¼å¼
4. å»ºè®®ä½¿ç”¨SSDå­˜å‚¨ä»¥æé«˜åŠ è½½é€Ÿåº¦
