#!/usr/bin/env python3
"""
æ‰¹å¤„ç†æœåŠ¡å™¨æµ‹è¯•å®¢æˆ·ç«¯
æ¼”ç¤ºå¦‚ä½•å‘æ‰¹å¤„ç†æœåŠ¡å™¨å‘é€å¤šä¸ªè¯·æ±‚æ¥è§¦å‘æ‰¹å¤„ç†
"""

import asyncio
import aiohttp
import json
import time
from typing import List
import sys


class BatchTestClient:
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def send_chat_completion(self, prompt: str, stream: bool = False) -> dict:
        """å‘é€å•ä¸ªèŠå¤©å®Œæˆè¯·æ±‚"""
        url = f"{self.base_url}/v1/chat/completions"

        payload = {
            "model": "test-model",
            "messages": [{"role": "user", "content": prompt}],
            "stream": stream,
            "temperature": 0.7,
            "max_tokens": 100,
        }

        request_start = time.time()

        if stream:
            return await self._handle_stream_request(url, payload, request_start)
        else:
            return await self._handle_non_stream_request(url, payload, request_start)

    async def _handle_non_stream_request(
        self, url: str, payload: dict, request_start: float
    ) -> dict:
        """å¤„ç†éæµå¼è¯·æ±‚"""
        async with self.session.post(url, json=payload) as response:
            if response.status == 200:
                result = await response.json()
                duration = time.time() - request_start
                return {
                    "success": True,
                    "duration": duration,
                    "response": result,
                    "content": result["choices"][0]["message"]["content"],
                }
            else:
                return {
                    "success": False,
                    "status": response.status,
                    "error": await response.text(),
                }

    async def _handle_stream_request(
        self, url: str, payload: dict, request_start: float
    ) -> dict:
        """å¤„ç†æµå¼è¯·æ±‚"""
        content_chunks = []
        first_chunk_time = None

        async with self.session.post(
            url, json=payload, headers={"Accept": "text/event-stream"}
        ) as response:
            if response.status != 200:
                return {
                    "success": False,
                    "status": response.status,
                    "error": await response.text(),
                }

            async for line in response.content:
                line = line.decode("utf-8").strip()
                if line.startswith("data: "):
                    data = line[6:]
                    if data == "[DONE]":
                        break

                    try:
                        chunk = json.loads(data)
                        if first_chunk_time is None:
                            first_chunk_time = time.time()

                        if chunk["choices"][0]["delta"].get("content"):
                            content_chunks.append(
                                chunk["choices"][0]["delta"]["content"]
                            )

                    except json.JSONDecodeError:
                        continue

            total_duration = time.time() - request_start
            first_chunk_latency = (
                first_chunk_time - request_start if first_chunk_time else None
            )

            return {
                "success": True,
                "duration": total_duration,
                "first_chunk_latency": first_chunk_latency,
                "content": "".join(content_chunks),
                "chunks_count": len(content_chunks),
            }

    async def test_single_request(self, prompt: str, stream: bool = False):
        """æµ‹è¯•å•ä¸ªè¯·æ±‚"""
        print(f"\n=== å•ä¸ªè¯·æ±‚æµ‹è¯• ({'æµå¼' if stream else 'éæµå¼'}) ===")
        print(f"å‘é€prompt: {prompt}")

        result = await self.send_chat_completion(prompt, stream)

        if result["success"]:
            print(f"âœ… è¯·æ±‚æˆåŠŸ")
            print(f"â±ï¸ æ€»è€—æ—¶: {result['duration']:.3f}s")
            if stream and result.get("first_chunk_latency"):
                print(f"ğŸš€ é¦–chunkå»¶è¿Ÿ: {result['first_chunk_latency']:.3f}s")
                print(f"ğŸ“¦ æ€»chunkæ•°: {result.get('chunks_count', 0)}")
            print(f"ğŸ’¬ å›å¤: {result['content'][:100]}...")
        else:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {result}")

    async def test_concurrent_requests(self, prompts: List[str], stream: bool = False):
        """æµ‹è¯•å¹¶å‘è¯·æ±‚ä»¥è§¦å‘æ‰¹å¤„ç†"""
        print(f"\n=== å¹¶å‘è¯·æ±‚æµ‹è¯• ({'æµå¼' if stream else 'éæµå¼'}) ===")
        print(f"å¹¶å‘å‘é€ {len(prompts)} ä¸ªè¯·æ±‚...")

        start_time = time.time()

        # å¹¶å‘å‘é€æ‰€æœ‰è¯·æ±‚
        tasks = [
            self.send_chat_completion(f"è¯·æ±‚{i+1}: {prompt}", stream)
            for i, prompt in enumerate(prompts)
        ]

        results = await asyncio.gather(*tasks)

        total_time = time.time() - start_time

        print(f"ğŸ¯ æ‰€æœ‰è¯·æ±‚å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.3f}s")

        successful = [r for r in results if r["success"]]
        failed = [r for r in results if not r["success"]]

        print(f"âœ… æˆåŠŸ: {len(successful)}, âŒ å¤±è´¥: {len(failed)}")

        if successful:
            avg_duration = sum(r["duration"] for r in successful) / len(successful)
            min_duration = min(r["duration"] for r in successful)
            max_duration = max(r["duration"] for r in successful)

            print(f"ğŸ“Š å•ä¸ªè¯·æ±‚è€—æ—¶ç»Ÿè®¡:")
            print(f"   å¹³å‡: {avg_duration:.3f}s")
            print(f"   æœ€å°: {min_duration:.3f}s")
            print(f"   æœ€å¤§: {max_duration:.3f}s")

            if stream:
                first_chunk_latencies = [
                    r["first_chunk_latency"]
                    for r in successful
                    if r.get("first_chunk_latency")
                ]
                if first_chunk_latencies:
                    avg_first_chunk = sum(first_chunk_latencies) / len(
                        first_chunk_latencies
                    )
                    print(f"ğŸš€ å¹³å‡é¦–chunkå»¶è¿Ÿ: {avg_first_chunk:.3f}s")

        # æ˜¾ç¤ºéƒ¨åˆ†å“åº”å†…å®¹
        print(f"\nğŸ“ å“åº”ç¤ºä¾‹:")
        for i, result in enumerate(successful[:3]):
            print(f"  è¯·æ±‚{i+1}: {result['content'][:80]}...")

    async def test_sequential_vs_concurrent(self, prompts: List[str]):
        """æ¯”è¾ƒé¡ºåºå¤„ç†å’Œå¹¶å‘å¤„ç†çš„æ€§èƒ½å·®å¼‚"""
        print(f"\n=== æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")

        # é¡ºåºå¤„ç†
        print(f"ğŸ”„ é¡ºåºå¤„ç† {len(prompts)} ä¸ªè¯·æ±‚...")
        sequential_start = time.time()
        sequential_results = []
        for i, prompt in enumerate(prompts):
            result = await self.send_chat_completion(f"é¡ºåº{i+1}: {prompt}")
            sequential_results.append(result)
        sequential_time = time.time() - sequential_start

        # å¹¶å‘å¤„ç†
        print(f"âš¡ å¹¶å‘å¤„ç† {len(prompts)} ä¸ªè¯·æ±‚...")
        concurrent_start = time.time()
        concurrent_tasks = [
            self.send_chat_completion(f"å¹¶å‘{i+1}: {prompt}")
            for i, prompt in enumerate(prompts)
        ]
        concurrent_results = await asyncio.gather(*concurrent_tasks)
        concurrent_time = time.time() - concurrent_start

        # ç»“æœå¯¹æ¯”
        print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æœ:")
        print(f"é¡ºåºå¤„ç†: {sequential_time:.3f}s")
        print(f"å¹¶å‘å¤„ç†: {concurrent_time:.3f}s")
        print(f"æ€§èƒ½æå‡: {(sequential_time/concurrent_time):.2f}x")

        # ååé‡è®¡ç®—
        sequential_throughput = len(prompts) / sequential_time
        concurrent_throughput = len(prompts) / concurrent_time
        print(f"é¡ºåºååé‡: {sequential_throughput:.2f} req/s")
        print(f"å¹¶å‘ååé‡: {concurrent_throughput:.2f} req/s")

    async def get_server_status(self):
        """è·å–æœåŠ¡å™¨çŠ¶æ€"""
        try:
            async with self.session.get(f"{self.base_url}/status") as response:
                if response.status == 200:
                    status = await response.json()
                    print(f"ğŸŸ¢ æœåŠ¡å™¨çŠ¶æ€: {status}")
                    return status
                else:
                    print(f"âŒ æ— æ³•è·å–æœåŠ¡å™¨çŠ¶æ€: {response.status}")
                    return None
        except Exception as e:
            print(f"âŒ è¿æ¥æœåŠ¡å™¨å¤±è´¥: {e}")
            return None


async def main():
    print("ğŸš€ æ‰¹å¤„ç†æœåŠ¡å™¨æµ‹è¯•å®¢æˆ·ç«¯å¯åŠ¨")
    print("=" * 50)

    async with BatchTestClient() as client:
        # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
        status = await client.get_server_status()
        if not status:
            print("è¯·ç¡®ä¿æœåŠ¡å™¨å·²å¯åŠ¨ (cargo run --bin batch_server)")
            return

        # æµ‹è¯•æ•°æ®
        test_prompts = [
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "è¯·è§£é‡Šæœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
            "æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
            "å¦‚ä½•ä¼˜åŒ–ç¥ç»ç½‘ç»œçš„æ€§èƒ½ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯transformeræ¶æ„ï¼Ÿ",
            "è¯·ä»‹ç»ä¸€ä¸‹æ³¨æ„åŠ›æœºåˆ¶",
        ]

        # 1. å•ä¸ªè¯·æ±‚æµ‹è¯•
        await client.test_single_request("æµ‹è¯•å•ä¸ªéæµå¼è¯·æ±‚", stream=False)
        await client.test_single_request("æµ‹è¯•å•ä¸ªæµå¼è¯·æ±‚", stream=True)

        # 2. å°æ‰¹é‡å¹¶å‘æµ‹è¯•ï¼ˆè§¦å‘æ‰¹å¤„ç†ï¼‰
        small_batch = test_prompts[:4]  # åˆšå¥½è¾¾åˆ°batch_size
        await client.test_concurrent_requests(small_batch, stream=False)
        await client.test_concurrent_requests(small_batch, stream=True)

        # 3. å¤§æ‰¹é‡å¹¶å‘æµ‹è¯•
        await client.test_concurrent_requests(test_prompts, stream=False)

        # 4. æ€§èƒ½å¯¹æ¯”æµ‹è¯•
        await client.test_sequential_vs_concurrent(test_prompts[:4])

        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("ğŸ’¡ è§‚å¯ŸæœåŠ¡å™¨æ—¥å¿—å¯ä»¥çœ‹åˆ°æ‰¹å¤„ç†çš„å·¥ä½œè¿‡ç¨‹")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æµ‹è¯•ä¸­æ–­")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å‡ºé”™: {e}")
