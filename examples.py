#!/usr/bin/env python3
"""
eLLM ä½¿ç”¨ç¤ºä¾‹ - å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Rust åç«¯çš„ Python API
"""

import sys
from pathlib import Path

# æ·»åŠ æœ¬åœ°åŒ…è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "python"))


def example_basic_usage():
    """åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹"""
    print("=== åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹ ===")

    from ellm import ModelArgs, Transformer

    # 1. åˆ›å»ºæ¨¡å‹é…ç½®
    config = ModelArgs(
        hidden_size=2048,  # éšè—å±‚å¤§å°
        num_hidden_layers=16,  # Transformer å±‚æ•°
        num_attention_heads=16,  # æ³¨æ„åŠ›å¤´æ•°
        vocab_size=32000,  # è¯æ±‡è¡¨å¤§å°
        max_seq_len=1024,  # æœ€å¤§åºåˆ—é•¿åº¦
    )

    print(f"æ¨¡å‹é…ç½®:")
    print(f"  - éšè—å±‚å¤§å°: {config.hidden_size}")
    print(f"  - å±‚æ•°: {config.num_hidden_layers}")
    print(f"  - æ³¨æ„åŠ›å¤´æ•°: {config.num_attention_heads}")
    print(f"  - è¯æ±‡è¡¨å¤§å°: {config.vocab_size}")
    print(f"  - æ³¨æ„åŠ›å¤´å¤§å°: {config.attention_head_size}")

    # 2. åˆ›å»º Transformer æ¨¡å‹
    model = Transformer(config)
    print(f"\næ¨¡å‹åˆ›å»ºæˆåŠŸ: {model}")

    # 3. æ¨¡æ‹Ÿæƒé‡åŠ è½½
    print("\nåŠ è½½æ¨¡æ‹Ÿæƒé‡...")
    mock_weights = {
        "model.embed_tokens.weight": "tensor_placeholder",
        "model.norm.weight": "tensor_placeholder",
        "lm_head.weight": "tensor_placeholder",
    }
    model.load_state_dict(mock_weights)

    # 4. å‰å‘ä¼ æ’­ç¤ºä¾‹
    print("\næ‰§è¡Œå‰å‘ä¼ æ’­...")
    input_sequences = [
        [1, 50, 100, 200],  # ç¬¬ä¸€ä¸ªåºåˆ—
        [1, 75, 150, 300, 400],  # ç¬¬äºŒä¸ªåºåˆ—
    ]

    try:
        logits = model.forward(input_sequences)
        print(f"å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {len(logits)} x {len(logits[0])}")
        print(f"ç¬¬ä¸€ä¸ªåºåˆ—çš„å‰5ä¸ªlogits: {logits[0][:5]}")
    except Exception as e:
        print(f"å‰å‘ä¼ æ’­å‡ºé”™: {e}")


def example_generation():
    """æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹"""
    print("\n=== æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ ===")

    from ellm import ModelArgs, Transformer

    # åˆ›å»ºå°æ¨¡å‹ç”¨äºå¿«é€Ÿæµ‹è¯•
    config = ModelArgs(
        hidden_size=512,
        num_hidden_layers=4,
        num_attention_heads=8,
        vocab_size=1000,
        max_seq_len=256,
    )

    model = Transformer(config)

    # åŠ è½½æ¨¡æ‹Ÿæƒé‡
    mock_weights = {"dummy": "weight"}
    model.load_state_dict(mock_weights)

    # ç”Ÿæˆæ–‡æœ¬
    prompt_tokens = [
        [1, 10, 20, 30],  # BOS + ä¸€äº› tokens
    ]

    print(f"è¾“å…¥æç¤º tokens: {prompt_tokens[0]}")

    try:
        generated = model.generate(
            prompt_tokens, max_gen_len=20, temperature=0.7, top_p=0.9
        )

        print(f"ç”Ÿæˆçš„å®Œæ•´åºåˆ—: {generated[0]}")
        print(f"æ–°ç”Ÿæˆçš„ tokens: {generated[0][len(prompt_tokens[0]):]}")
    except Exception as e:
        print(f"ç”Ÿæˆå‡ºé”™: {e}")


def example_config_loading():
    """ä»é…ç½®æ–‡ä»¶åŠ è½½ç¤ºä¾‹"""
    print("\n=== é…ç½®æ–‡ä»¶åŠ è½½ç¤ºä¾‹ ===")

    from ellm import ModelArgs, create_transformer_from_config
    import json
    import tempfile
    import os

    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
    config_data = {
        "hidden_size": 4096,
        "vocab_size": 32000,
        "num_hidden_layers": 32,
        "num_attention_heads": 32,
        "num_key_value_heads": 32,
        "intermediate_size": 11008,
        "max_position_embeddings": 2048,
        "rms_norm_eps": 1e-6,
        "bos_token_id": 1,
        "eos_token_id": 2,
        "model_type": "llama",
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
        json.dump(config_data, f, indent=2)
        config_path = f.name

    try:
        # ä»æ–‡ä»¶åŠ è½½é…ç½®
        model_args = ModelArgs.from_config_file(config_path)
        print(f"ä»é…ç½®æ–‡ä»¶åŠ è½½: {config_path}")
        print(f"é…ç½®: {model_args.hidden_size}x{model_args.num_hidden_layers}")

        # åˆ›å»ºæ¨¡å‹
        model = create_transformer_from_config(
            config_path,
            max_seq_len=1024,  # è¦†ç›–é…ç½®å‚æ•°
        )
        print(f"æ¨¡å‹åˆ›å»ºæˆåŠŸ: {model.model_args.max_seq_len}")

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(config_path)


def example_high_level_api():
    """é«˜çº§ API ç¤ºä¾‹"""
    print("\n=== é«˜çº§ API ç¤ºä¾‹ ===")

    try:
        from ellm import Llama

        # æ³¨æ„ï¼šè¿™éœ€è¦å®é™…çš„æ¨¡å‹æ–‡ä»¶ï¼Œè¿™é‡Œåªæ˜¯æ¼”ç¤º API
        print("é«˜çº§ Llama API æ¼”ç¤º (éœ€è¦å®é™…æ¨¡å‹æ–‡ä»¶):")
        print("""
        # åˆå§‹åŒ– Llama æ¨¡å‹
        llama = Llama(
            model_path="path/to/model",
            tokenizer_path="path/to/tokenizer",
            max_seq_len=2048
        )
        
        # ç®€å•æ–‡æœ¬ç”Ÿæˆ
        response = llama.generate(
            "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
            max_gen_len=100,
            temperature=0.7
        )
        
        # èŠå¤©å¯¹è¯
        messages = [
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
        ]
        response = llama.chat_completion(messages)
        """)

    except ImportError as e:
        print(f"é«˜çº§ API å¯¼å…¥å¤±è´¥: {e}")


def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸš€ eLLM Python API ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)

    try:
        example_basic_usage()
        example_generation()
        example_config_loading()
        example_high_level_api()

        print("\n" + "=" * 50)
        print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
        print("\nè¦æ„å»ºå’Œå®‰è£…å®Œæ•´çš„ Rust æ‰©å±•ï¼Œè¯·è¿è¡Œ:")
        print("  python build.py")

    except Exception as e:
        print(f"\nâŒ ç¤ºä¾‹è¿è¡Œå‡ºé”™: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
