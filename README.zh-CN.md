# eLLM：是一款运行在单路 CPU-only 服务器上的大语言模型推理框架：
- 运行满血MoE大模型（Qwen3-480B），具备实时短文本推理能力（100ms/token）
- 支持百万 tokens 的长文本深度思考与理解

实现原理：
- 利用 CPU 的大内存，通过空间换时间实现极致推理性能

🌐 语言版本：[English](README.md) | [简体中文](README.zh-CN.md)

## ✅ 重要提示
* 项目正在积极开发中，预计将在 **约 1 个月后** 发布最小原型（Qwen30B）！  
* 我们目前正在寻找志愿者——如果你感兴趣，请联系 **lucienhuangfu@outlook.com**。

**关键能力**：
* 支持 MoE 模型完整加载，动态专家激活
* 完整支持百万 tokens 上下文 (KV Cache)
* 标准 Attention 推理（token 与全文深度关联）

## 应用场景 1：在线短文本推理
* 搜索问答
* 代码补全
* 客服对话

## 应用场景 2：离线长文本推理（Deep Research）
- 代码审计 / 高危漏洞查找
- 合同审核 / 文档审查  
- 财务报表合规检查
- 文学作品创作 / 延续式写作 

## 竞品对比：eLLM 降低大模型普及门槛

**eLLM 让中小团队也能轻松部署大模型，成本更低、部署更灵活。**

### 无需高性能 GPU 服务器
- 单路CPU-only服务器即可运行 MoE 架构大模型  
- 仅需支持 AVX512-F16 的通用 CPU  
- 可通过加装 DDR5 扩展内存 

### 部署简单，适配多种场景
- 本地服务器 / 私有云 / 边缘节点轻松部署  
- 支持按需弹性计算，任务完成后自动释放资源  
- 横向扩展架构，满足高并发推理需求

机器对比：CPU-only 服务器 vs GPU 服务器

| CPU-only 服务器 | 条目 | GPU 服务器 | |
|----------|--------------|------------|------|
|CPU ||CPU|GPU| 
| Xeon 6900| 型号           |   Xeon 8480+     | H20   |
|3|内存容量(TB)|2|0.141|
| 1| 数量          |4        | 8  |
|15|总价(万元) |150| 


## 现有方案存在如下问题
- 高门槛、高成本
  - 🧠 **GPU 推理门槛高**：每位用户的长文本推理成本极高
  - 📦 **上下文受限**：GPU 显存无法容纳完整长文本上下文
  - 🔀 **专家切分复杂**：需要同步专家路由，增加系统复杂度
- 动态内存管理和任务生成需要额外占用CPU
  - CPU-only 服务器需要占用小半块自己的CPU，还剩大半块CPU专注计算任务
  - GPU 服务器需要占用host CPU, GPU可以专注计算任务
- 性能瓶颈随长度放大（超线性增长）
  - 动态内存分配：提前消耗内存，不可控
  - 动态图构建：运行时开销大，效率低
  - Chunked KV 缓存：难以利用完整带宽，降低推理效率


## 💡 为什么 MoE 更适合 CPU-only 推理？

- MoE 是“存大算小”的大模型架构
  - 存储需求大：TB 级专家参数需常驻内存  
  - 通信需求小：仅加载激活专家，带宽要求低  
  - 计算需求小：仅计算激活路径，算力开销小  
- CPU-only 架构正好契合 MoE 推理需求
  - **大内存容量**：轻松容纳全部专家参数  
  - **小带宽需求**：激活专家少，内存带宽压力低
    - **MRDIMM**：内存带宽翻倍，填满算力管线  
  - **小算力需求**：适配 AMX 的矩阵计算场景
    - **AMX**：矩阵指令扩展，计算性能数倍提升


## 为什么 eLLM 优于现有框架（vLLM）？

### 空间换时间：重构 CPU 推理引擎的核心逻辑

#### 🛠️ 静态资源分配
- 静态内存分配 + 静态图编译 -> 静态任务集
- 直接获得任务，释放更多 CPU 用于计算

#### ⚡ 推理延迟随长度**线性增长**
- 静态分配内存：避免碎片化，获得全局最优布局  
- 连续内存布局：KV Cache 带宽利用最大化  
- 动态任务调度：支持不规则输入  
- 动态激活专家：仅计算必要路径，节省算力


## 路线图
* [ ] Qwen (30B，480B)（即将支持）
* [x] LLaMA 2 / 3
* [ ] DeepSeek（即将支持）
* [ ] gpt-oss


## 📄 论文

如果你对技术细节感兴趣，可以阅读我们的论文并引用：

```bibtex
@article{ellm2025,
  title={eLLM: Achieving Lossless Million-Token LLM Inference on CPUs Faster Than GPUs},
  author={Yaguang Huangfu},
  journal={preprint https://www.researchgate.net/publication/393416965},
  year={2025}
}
```

## 📜 开源协议
这个项目使用 [Apache 2.0 License](LICENSE).