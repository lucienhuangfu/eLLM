#!/usr/bin/env python3
"""
æµ‹è¯• eLLM å®‰è£…å’ŒåŸºæœ¬åŠŸèƒ½
"""


def test_import():
    """æµ‹è¯•æ¨¡å—å¯¼å…¥"""
    print("Testing imports...")

    try:
        import ellm

        print(f"âœ“ ellm imported successfully, version: {ellm.__version__}")
    except ImportError as e:
        print(f"âŒ Failed to import ellm: {e}")
        return False

    try:
        from ellm import ModelArgs, Transformer, Llama

        print("âœ“ Main classes imported successfully")
    except ImportError as e:
        print(f"âŒ Failed to import main classes: {e}")
        return False

    return True


def test_config():
    """æµ‹è¯•é…ç½®åŠŸèƒ½"""
    print("\nTesting configuration...")

    try:
        from ellm import ModelArgs

        # æµ‹è¯•é»˜è®¤é…ç½®
        config = ModelArgs()
        print(
            f"âœ“ Default config created: {config.hidden_size}x{config.num_hidden_layers}"
        )

        # æµ‹è¯•è‡ªå®šä¹‰é…ç½®
        custom_config = ModelArgs(
            hidden_size=2048, num_hidden_layers=16, vocab_size=50000
        )
        print(
            f"âœ“ Custom config created: {custom_config.hidden_size}x{custom_config.num_hidden_layers}"
        )

        return True
    except Exception as e:
        print(f"âŒ Config test failed: {e}")
        return False


def test_transformer():
    """æµ‹è¯• Transformer åŠŸèƒ½"""
    print("\nTesting Transformer...")

    try:
        from ellm import ModelArgs, Transformer

        # åˆ›å»ºå°æ¨¡å‹ç”¨äºæµ‹è¯•
        config = ModelArgs(
            hidden_size=512, num_hidden_layers=4, num_attention_heads=8, vocab_size=1000
        )

        model = Transformer(config)
        print(f"âœ“ Transformer created: {model}")

        # æµ‹è¯•æ¨¡æ‹Ÿçš„å‰å‘ä¼ æ’­ï¼ˆåœ¨æœªåŠ è½½æƒé‡æ—¶ä¼šå¤±è´¥ï¼Œè¿™æ˜¯é¢„æœŸçš„ï¼‰
        try:
            sequences = [[1, 2, 3, 4, 5]]
            logits = model.forward(sequences)
            print("âŒ Forward pass should have failed without weights")
            return False
        except RuntimeError as e:
            if "weights must be loaded" in str(e):
                print("âœ“ Forward pass correctly failed without weights")
            else:
                print(f"âŒ Unexpected error: {e}")
                return False

        return True
    except Exception as e:
        print(f"âŒ Transformer test failed: {e}")
        return False


def test_rust_bindings():
    """æµ‹è¯• Rust ç»‘å®šï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
    print("\nTesting Rust bindings...")

    try:
        from ellm._lowlevel import Config, Transformer as RustTransformer

        # æµ‹è¯• Rust Config
        rust_config = Config(
            hidden_size=512, vocab_size=1000, num_hidden_layers=4, num_attention_heads=8
        )
        print(f"âœ“ Rust Config created: {rust_config}")

        # æµ‹è¯• Rust Transformer
        rust_model = RustTransformer(rust_config)
        print(f"âœ“ Rust Transformer created: {rust_model}")

        return True
    except ImportError:
        print("âš ï¸  Rust bindings not available (module not compiled)")
        return True  # è¿™ä¸æ˜¯é”™è¯¯ï¼Œåªæ˜¯æ¨¡å—æœªç¼–è¯‘
    except Exception as e:
        print(f"âŒ Rust bindings test failed: {e}")
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ§ª Testing eLLM Installation")
    print("=" * 40)

    tests = [test_import, test_config, test_transformer, test_rust_bindings]

    passed = 0
    total = len(tests)

    for test in tests:
        if test():
            passed += 1
        print()

    print("=" * 40)
    print(f"Tests passed: {passed}/{total}")

    if passed == total:
        print("ğŸ‰ All tests passed!")
        return True
    else:
        print("âŒ Some tests failed")
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
